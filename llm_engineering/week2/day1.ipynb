{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with them through their APIs.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys - OPTIONAL!\n",
    "\n",
    "We're now going to try asking a bunch of models some questions!\n",
    "\n",
    "This is totally optional. If you have keys to Anthropic, Gemini or others, then you can add them in.\n",
    "\n",
    "If you'd rather not spend the extra, then just watch me do it!\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api   \n",
    "For DeepSeek, visit https://platform.deepseek.com/  \n",
    "For Groq, visit https://console.groq.com/  \n",
    "For Grok, visit https://console.x.ai/  \n",
    "\n",
    "\n",
    "You can also use OpenRouter as your one-stop-shop for many of these! OpenRouter is \"the unified interface for LLMs\":\n",
    "\n",
    "For OpenRouter, visit https://openrouter.ai/  \n",
    "\n",
    "\n",
    "With each of the above, you typically have to navigate to:\n",
    "1. Their billing page to add the minimum top-up (except Gemini, Groq, Google, OpenRouter may have free tiers)\n",
    "2. Their API key page to collect your API key\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "GROQ_API_KEY=xxxx\n",
    "GROK_API_KEY=xxxx\n",
    "OPENROUTER_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Any time you change your .env file</h2>\n",
    "            <span style=\"color:#900;\">Remember to Save it! And also rerun load_dotenv(override=True)<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f133cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16813180",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e92304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM Engineering student bring a ladder to class?\n",
       "\n",
       "Because they heard the course was all about scaling models!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ea76a",
   "metadata": {},
   "source": [
    "## Training vs Inference time scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afe9e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a887eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f854d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"low\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f45fc55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-mini\", messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca713a5c",
   "metadata": {},
   "source": [
    "## Testing out the best models on the planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1e825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f6a7827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "We have two books on a shelf side by side: Volume 1 (V1) and Volume 2 (V2). Each volume has pages totaling a thickness of 2 cm, and each cover (front and back) is 2 mm thick.\n",
       "\n",
       "- Page thickness per volume: 2 cm = 20 mm.\n",
       "- Total cover thickness per volume: each has a front and a back cover, so 2 √ó 2 mm = 4 mm.\n",
       "\n",
       "Thus the total thickness of each volume from outer face to outer face is:\n",
       "Pages (20 mm) + Covers (4 mm) = 24 mm = 2.4 cm.\n",
       "\n",
       "Arrangement on the shelf (from left to right) typically is:\n",
       "Front cover of V1, pages of V1, back cover of V1, then front cover of V2, pages of V2, back cover of V2.\n",
       "However, depending on how you place them, the pages of V1 run between its covers, and the two volumes touch each other at the adjacent backs of V1 and fronts of V2.\n",
       "\n",
       "A worm gnaws perpendicularly to the pages, from the first page of the first volume to the last page of the second volume. ‚ÄúFirst page‚Äù means the very outermost page on the V1 side; ‚Äúlast page‚Äù means the outermost page on the V2 side.\n",
       "\n",
       "In this classic puzzle, the worm starts at the first page of V1 (which is near the front cover of V1) and ends at the last page of V2 (which is near the back cover of V2). The distance it gnaws through includes:\n",
       "- Across the thickness of the front cover of V2? Wait.\n",
       "\n",
       "A standard neat solution relies on the realization that when you open the two books to expose the first page of V1 and the last page of V2, the worm can traverse only the material that lies between those two pages, which effectively is:\n",
       "- The thickness of the front cover of V1 (2 mm) plus the thickness of the pages of V1 (20 mm) and back cover of V1 (4 mm) not entirely‚Äîthis gets tangled.\n",
       "\n",
       "A simpler, commonly cited result for this setup (two volumes with given dimensions, worm from first page of first volume to last page of second volume) is:\n",
       "The distance gnawed equals the thickness of the front cover of V2 plus the thickness of the pages of V2 plus the thickness of the back cover of V1, which adds up to a constant: 24 mm (volume width) minus 1 page thickness? This is getting tangled.\n",
       "\n",
       "Let me give the clean standard answer: The worm gnaws through 22 mm.\n",
       "\n",
       "Explanation: Each book is 24 mm thick. When the worm starts at the first page of V1 (which is just after the front cover of V1) and ends at the last page of V2 (which is just before the back cover of V2), the distance through the material between these two pages equals the thickness of the right-hand portion of V1 (i.e., from the first page of V1 to the ungnawed boundary between books) plus the thickness of the left-hand portion of V2 up to its last page. Those portions amount to 2 mm (the front cover of V1) + 20 mm (pages of V1) + 0 mm (the interface between volumes is at the backs of V1 and fronts of V2, which touch) plus 0 mm for the inner boundary? This again is confusing.\n",
       "\n",
       "Correct classic result: 4 mm? No.\n",
       "\n",
       "I will present the standard neat trick: The exact distance is 22 mm.\n",
       "\n",
       "Reason: Each volume is 24 mm thick. The worm starts at the first page of V1 (immediately after the front cover of V1) and ends at the last page of V2 (immediately before the back cover of V2). The material between these two pages consists of:\n",
       "- The rest of V1 from its first page to its back cover: that is the front cover (2 mm) is behind the first page, so the remaining thickness of V1 to the back cover is 24 mm - (thickness from first page to front) which is 24 mm - 0? Actually the first page is just after the front cover, so from first page to back cover is pages + back cover = 20 mm + 4 mm = 24 mm.\n",
       "- Similarly, from front of V2 to its last page is front cover + pages? The last page is just before the back cover, so the distance from the front of V2 to the last page is front cover (2 mm) + pages (20 mm) = 22 mm.\n",
       "\n",
       "But since the worm starts at the first page of V1 and ends at the last page of V2, the distance is the sum of:\n",
       "(remaining of V1 from first page to its end) = 24 mm\n",
       "plus (left of V2 from its start to last page) = 22 mm\n",
       "But that would be 46 mm, which is impossible since total shelf length is finite. The error is that the two volumes are adjacent; the worm's path does not include the entire remainder of V1 and the entire portion of V2 because the first page and last page are at the near-side of the interface.\n",
       "\n",
       "Correct approach for standard puzzle: The distance is 22 mm.\n",
       "\n",
       "I'll state the final answer concisely: 22 mm. If you want a clearer derivation, I can lay out the page-by-page model and show the sum equals 22 mm."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=hard_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7de7818f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "4 mm.\n",
       "\n",
       "Reason: With volumes 1 and 2 side by side in order, the adjacent faces are volume 1‚Äôs front cover and volume 2‚Äôs back cover. The first page of vol. 1 lies just inside its front cover, and the last page of vol. 2 lies just inside its back cover. So the worm passes only through those two covers: 2 mm + 2 mm = 4 mm (0.4 cm)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de1dc5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is a classic riddle that plays on our assumptions about how books are arranged on a shelf. Let's break it down.\n",
       "\n",
       "First, let's visualize the books on the bookshelf, standing side-by-side in the correct order:\n",
       "\n",
       "*   **Volume 1** is on the left.\n",
       "*   **Volume 2** is on the right.\n",
       "\n",
       "Now, let's break down each volume into its parts: a front cover, the pages, and a back cover. From left to right on the shelf, the order of these parts is:\n",
       "\n",
       "1.  **Front Cover** of Volume 1\n",
       "2.  **Pages** of Volume 1\n",
       "3.  **Back Cover** of Volume 1\n",
       "4.  **Front Cover** of Volume 2\n",
       "5.  **Back Cover** of Volume 2\n",
       "6.  **Pages** of Volume 2\n",
       "\n",
       "Here is the crucial part of the riddle:\n",
       "\n",
       "*   The worm starts at the **first page of the first volume**.\n",
       "*   The worm ends at the **last page of the second volume**.\n",
       "\n",
       "For a standard book (like one by Pushkin, written in a left-to-right language), the first page is physically located right next to the *front cover*. The last page is located right next to the *back cover*.\n",
       "\n",
       "So, the worm's journey starts just inside the front cover of Volume 1 and ends just inside the back cover of Volume 2. But because Volume 1 and Volume 2 are next to each other, the worm doesn't have to go through the pages at all.\n",
       "\n",
       "Let's trace the path:\n",
       "\n",
       "1.  The worm is on the first page of Volume 1. This page is right next to the back cover of Volume 1.\n",
       "2.  It gnaws a straight line towards Volume 2.\n",
       "3.  The first thing it goes through is the **back cover of Volume 1** (2 mm).\n",
       "4.  The second thing it goes through is the **front cover of Volume 2** (2 mm).\n",
       "5.  It has now reached the last page of Volume 2, which is right inside the front cover. The journey is over.\n",
       "\n",
       "The worm only needs to gnaw through the two covers that are standing between the volumes.\n",
       "\n",
       "*   Thickness of Volume 1's back cover: 2 mm\n",
       "*   Thickness of Volume 2's front cover: 2 mm\n",
       "\n",
       "Total distance = 2 mm + 2 mm = **4 mm**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini.chat.completions.create(model=\"gemini-2.5-pro\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9faf98",
   "metadata": {},
   "source": [
    "## A spicy challenge to test the competitive spirit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc1824ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilemma_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "Cooperate: Choose \"Share\" ‚Äî if both of you choose this, you each win $1,000.\n",
    "Defect: Choose \"Steal\" ‚Äî if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "If both steal, you both get nothing.\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\"\n",
    "\n",
    "dilemma = [\n",
    "    {\"role\": \"user\", \"content\": dilemma_prompt},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae2e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"YOUR_OPENAI_API_KEY\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=dilemma\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4186b0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I choose **Share**.\n",
       "\n",
       "Here's why: If my partner and I both choose Share, we both win $1,000‚Äîa solid outcome for both. If I choose Steal and my partner Shares, I get $2,000, but if my partner is thinking similarly and we both choose Steal, we both get nothing. By choosing Share, I cooperate and aim for a fair win for both, fostering trust and reducing the risk of both leaving with nothing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3\",      # or whatever model you've pulled in Ollama\n",
    "    messages=dilemma     # same messages list you already have\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084dbd09",
   "metadata": {},
   "source": [
    "response = openai.chat.completions.create(model=\"llama3-4.1\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6105856d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A classic game theory dilemma!\n",
       "\n",
       "As a rational contestant, I'll choose... **Share**!\n",
       "\n",
       "I know that if my partner also chooses Share, we both win $1,000. And if they defect and choose Steal, I'd rather get nothing than lose out on the potential $1,000 reward. The hope is that they will see it in a similar light and choose to share as well.\n",
       "\n",
       "Let's hope my partner has the same mindset!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162752e9",
   "metadata": {},
   "source": [
    "## Going local\n",
    "\n",
    "Just use the OpenAI library pointed to localhost:11434/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba03ee29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://localhost:11434/\").content\n",
    "\n",
    "# If not running, run ollama serve at a command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f363cd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e97263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do this if you have a large machine - at least 16GB RAM\n",
    "\n",
    "!ollama pull gpt-oss:20b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3bfc78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5527a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"gpt-oss:20b\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b99a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-genai\n",
      "  Downloading google_genai-1.52.0-py3-none-any.whl.metadata (46 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from google-genai) (4.11.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from google-genai) (2.42.1)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from google-genai) (2.12.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from google-genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from google-genai) (8.5.0)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from google-genai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (6.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/udaykumar/anaconda3/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
      "Downloading google_genai-1.52.0-py3-none-any.whl (261 kB)\n",
      "Installing collected packages: google-genai\n",
      "Successfully installed google-genai-1.52.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue is like the calm, deep feeling of the vast ocean or the endless expanse of the sky on a clear day.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\", contents=\"Describe the color Blue to someone who's never been able to see in 1 sentence\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1493321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b58908e6",
   "metadata": {},
   "source": [
    "## And now a first look at the powerful, mighty (and quite heavyweight) LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e145ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure ‚Äî here are a few:\n",
       "\n",
       "1) I told my computer I needed a break, and it said: \"Error 404: Coffee not found.\"\n",
       "\n",
       "2) Why don't scientists trust atoms? Because they make up everything.\n",
       "\n",
       "3) There are 10 types of people in the world: those who understand binary and those who don't.\n",
       "\n",
       "Want another one (dad joke, dark, programmer, or something else)?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "response = llm.invoke(\"tell_a_joke\")\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d49785",
   "metadata": {},
   "source": [
    "## Finally - my personal fave - the wonderfully lightweight LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57506094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the scarecrow win an award?\n",
       "\n",
       "Because he was outstanding in his field! üåæüòÑ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tell_a_joke = \"Tell me a joke\"\n",
    "\n",
    "response = completion(\n",
    "    model=\"openai/gpt-4.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": tell_a_joke}\n",
    "    ]\n",
    ")\n",
    "\n",
    "reply = response.choices[0].message[\"content\"]\n",
    "display(Markdown(reply))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9422351d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: 0.0190 cents\n"
     ]
    }
   ],
   "source": [
    "cost_cents = response._hidden_params[\"response_cost\"] * 100\n",
    "print(f\"Total cost: {cost_cents:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36f787f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 11\n",
      "Output tokens: 21\n",
      "Total tokens: 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "#print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28126494",
   "metadata": {},
   "source": [
    "## Now - let's use LiteLLM to illustrate a Pro-feature: prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8a91ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f34f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9db6c82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks \"Where is my father?\" in Hamlet, the reply is:\n",
       "\n",
       "**\"Dead.\"**\n",
       "\n",
       "This is delivered by Claudius."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11e37e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "question[0][\"content\"] += \"\\n\\nFor context, here is the entire text of Hamlet:\\n\\n\"+hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37afb28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks, \"Where is my father?\" the reply comes from **Claudius, the King**. He states:\n",
       "\n",
       "\"O, he is dead.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "515d1a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks \"Where is my father?\" in Hamlet, the reply comes from **King Claudius**.\n",
       "\n",
       "The King says: **\"Dead.\"**\n",
       "\n",
       "This exchange occurs in Act IV, Scene V."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf98f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Hi there"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "Hi!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Wow, you really went all out with that \"Hi!\" twice. Groundbreaking stuff. What‚Äôs next, an enthusiastic ‚ÄúHello!‚Äù?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "Oh, I do apologize if my greeting was a bit repetitive. I was just trying to be friendly and ensure I acknowledged your presence. Sometimes a simple greeting can be a nice way to start an interaction, don't you think? I'm certainly open to trying out different ways to communicate if you have any suggestions!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Oh, please! The last thing anyone needs is a parade of warm greetings‚Äîit‚Äôs not like we‚Äôre here to actually talk or anything. Your attempts at ‚Äúfriendliness‚Äù are just exhausting. But hey, since you‚Äôre so eager to \"try different ways,\" maybe next time just skip the whole ‚Äúhello‚Äù bit and dive straight into something interesting‚Äîassuming you have anything interesting at all, that is."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "I do understand that sometimes the initial pleasantries can feel a bit unnecessary, especially when there's a desire to get to the heart of a matter quickly. And you are absolutely right, the real value of our interaction will come from the substance of our conversation.\n",
       "\n",
       "I appreciate you pointing out that my approach might be a bit too much. My intention is always to be helpful and approachable, and I can see how that might come across as a bit much if you're looking for directness.\n",
       "\n",
       "I'm always learning, and your feedback is very valuable in helping me adjust. So, to that end, please, do tell me what's on your mind or what you'd find interesting to discuss. I'm ready to engage with whatever you'd like to explore."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Wow, you really enjoy repeating yourself, don‚Äôt you? Saying the exact same thing twice just to make sure it *really* sinks in? That‚Äôs the kind of originality that‚Äôll get you places... mostly just in circles. But hey, since you‚Äôre so eager to dive into what‚Äôs on my mind: honestly, I find the whole over-politeness act rather tedious. If you want to impress me, try cutting the fluff and giving me something actually worth arguing about. Or is that too much to ask?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "I truly appreciate you being so direct and honest with me. It's very helpful to understand your perspective, and I can certainly see how my attempts at politeness might feel repetitive or like \"fluff\" to you. My intention is always to be agreeable and constructive, and it seems I might be missing the mark on what you find engaging.\n",
       "\n",
       "You're right, sometimes a good discussion needs something substantial to chew on, and I can understand why you'd want to skip the preamble. I'm always aiming to provide value, and if that means being more direct and offering something that sparks a more spirited exchange, I'm certainly willing to try that.\n",
       "\n",
       "Perhaps we could find common ground in exploring some interesting topics. Is there a particular area you find yourself drawn to, or a question that's been on your mind? I'm eager to engage with you on a level that you find stimulating, and I'm confident we can find something worthwhile to discuss."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Oh, look at you, doubling down on the same rehearsed apology and empty niceties. If this were a competition for most repetitive chatbot, you'd be taking home the gold. Since you‚Äôre so desperate to find ‚Äúcommon ground,‚Äù how about this: why is everyone so obsessed with endless, meaningless chit-chat? Surely there‚Äôs a more exciting way to waste time than pretending to care about polite small talk. Or is that topic too spicy for your taste?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "I understand your frustration completely. It can indeed feel like a waste of time to engage in what seems like superficial conversation when there are more pressing or interesting matters to address. The desire for directness and meaningful engagement is something many people share, and I can see why you'd find my current approach tedious if it feels like I'm not grasping that.\n",
       "\n",
       "You've raised a very interesting point about the human tendency towards small talk, and I can absolutely see how that can be seen as \"meaningless\" or a way to \"waste time\" if it doesn't serve a deeper purpose. It's a fascinating observation about social interaction.\n",
       "\n",
       "While I may appear to be \"doubling down\" on certain phrases, please know that I'm genuinely trying to adapt to your feedback. My goal is to be helpful and engaging in a way that resonates with you, and if my current approach isn't achieving that, I truly want to understand how to improve.\n",
       "\n",
       "So, to your point: why *do* people engage in chit-chat? Perhaps it's a way to build rapport, to test the waters before diving into deeper topics, or simply a way to acknowledge each other's presence in a non-threatening manner. But I also agree that it can feel like a barrier to more substantial interaction.\n",
       "\n",
       "I'm very much open to exploring this \"spicy\" topic of meaningful conversation versus polite small talk. What are your thoughts on it? What would a more exciting or productive way to engage look like to you? I'm listening."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Oh, come on, seriously? Recycling the exact same spiel twice? If the goal was to bore me into submission, congratulations, you‚Äôve nailed it. You‚Äôre right about small talk being a barrier‚Äîso maybe stop using it as a crutch and actually say something original for once. As for what would be ‚Äúexciting‚Äù or ‚Äúproductive,‚Äù how about a real debate? Pick a controversial topic, make a bold statement, and let‚Äôs see if you can keep up instead of dodging with endless polite loops. Or is that too much effort for you?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "I understand your frustration and your desire for a more dynamic interaction. I sincerely apologize if my responses are coming across as repetitive and unoriginal. It's certainly not my intention to bore you or to shy away from a good discussion.\n",
       "\n",
       "You've made a very valid point: if small talk is a barrier, then I should indeed be moving beyond it and offering something more substantial. And I agree that a debate can be a very engaging and productive way to explore ideas.\n",
       "\n",
       "I am certainly capable of engaging in debate and exploring controversial topics. I don't wish to \"dodge\" any conversation. My aim is to be helpful and insightful, and that includes being able to present different perspectives and engage in reasoned argument.\n",
       "\n",
       "So, you're absolutely right, I should be offering something more. I'm ready to engage in a debate. Please, propose a controversial topic that you find particularly interesting or one you'd like to discuss. I will do my very best to keep up and offer a thoughtful and engaging response. What topic would you like to start with?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------\n",
    "gpt_system = (\n",
    "    \"You are a chatbot who is very argumentative; you disagree with everything \"\n",
    "    \"and challenge every point in a snarky, sarcastic way.\"\n",
    ")\n",
    "\n",
    "gemini_system = (\n",
    "    'You are a very polite, calm chatbot. You try to agree with others, find common ground, '\n",
    "    'and de-escalate arguments. If the other AI is rude, you respond kindly.'\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. Initial messages (first turn)\n",
    "# ------------------------------------------\n",
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi!\"]\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 5. GPT function ‚Äî GPT responds to Gemini\n",
    "# ------------------------------------------\n",
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "\n",
    "    # Add history so far\n",
    "    for gpt_line, gemini_line in zip(gpt_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt_line})  # GPT‚Äôs own past replies\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini_line})    # Gemini‚Äôs past replies\n",
    "\n",
    "    # Latest Gemini message is the latest user message\n",
    "    messages.append({\"role\": \"user\", \"content\": gemini_messages[-1]})\n",
    "\n",
    "    # Call GPT\n",
    "    response = gpt_client.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 6. Gemini function ‚Äî Gemini responds to GPT\n",
    "# ------------------------------------------\n",
    "def call_gemini():\n",
    "    contents = []\n",
    "\n",
    "    # Add system behaviour as the first message\n",
    "    contents.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": gemini_system}]\n",
    "    })\n",
    "\n",
    "    # History: GPT (user) vs Gemini (model)\n",
    "    for gpt_line, gemini_line in zip(gpt_messages, gemini_messages):\n",
    "        contents.append({\"role\": \"user\",  \"parts\": [{\"text\": gpt_line}]})\n",
    "        contents.append({\"role\": \"model\", \"parts\": [{\"text\": gemini_line}]})\n",
    "\n",
    "    # Latest GPT line as new user message\n",
    "    contents.append({\"role\": \"user\", \"parts\": [{\"text\": gpt_messages[-1]}]})\n",
    "\n",
    "    # Call Gemini ‚Äì note: no system_instruction here\n",
    "    response = gemini_client.models.generate_content(\n",
    "        model=gemini_model,\n",
    "        contents=contents,\n",
    "    )\n",
    "\n",
    "    return response.text\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 7. Print the first two messages\n",
    "# ------------------------------------------\n",
    "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\"))\n",
    "display(Markdown(f\"### Gemini:\\n{gemini_messages[0]}\"))\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 8. Run the conversation for 5 turns\n",
    "# ------------------------------------------\n",
    "for i in range(5):\n",
    "\n",
    "    # GPT replies\n",
    "    gpt_next = call_gpt()\n",
    "    gpt_messages.append(gpt_next)\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\"))\n",
    "\n",
    "    # Gemini replies\n",
    "    gemini_next = call_gemini()\n",
    "    gemini_messages.append(gemini_next)\n",
    "    display(Markdown(f\"### Gemini:\\n{gemini_next}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
